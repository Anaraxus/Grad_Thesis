{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRZ02xMb54OHFwHETGAT+B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-PbmM0WCEcf7"},"source":["import numpy as np\n","\n","from keras import backend\n","from keras.layers import Activation, Input\n","from keras.layers.core import Dense, Lambda, Reshape\n","from keras.layers.convolutional import Convolution1D\n","from keras.layers.merge import concatenate, dot\n","from keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1uXsHCM3TsS8"},"source":["LETTER_GRAM_SIZE = 3 # See section 3.2.\n","WINDOW_SIZE = 3 # See section 3.2.\n","TOTAL_LETTER_GRAMS = int(3 * 1e4) # Determined from data. See section 3.2.\n","WORD_DEPTH = WINDOW_SIZE * TOTAL_LETTER_GRAMS # See equation (1).\n","K = 300 # Dimensionality of the max-pooling layer. See section 3.4.\n","L = 128 # Dimensionality of latent semantic space. See section 3.5.\n","J = 4 # Number of random unclicked documents serving as negative examples for a query. See section 4.\n","FILTER_LENGTH = 1 # We only consider one time step for convolutions."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jraIpe6Sn4b"},"source":["# Input tensors holding the query, positive (clicked) document, and negative (unclicked) documents.\n","# The first dimension is None because the queries and documents can vary in length.\n","query = Input(shape = (None, WORD_DEPTH))\n","pos_doc = Input(shape = (None, WORD_DEPTH))\n","neg_docs = [Input(shape = (None, WORD_DEPTH)) for j in range(J)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8IHJQwZOStCQ"},"source":["# Query model. The paper uses separate neural nets for queries and documents (see section 5.2).\n","\n","# In this step, we transform each word vector with WORD_DEPTH dimensions into its\n","# convolved representation with K dimensions. K is the number of kernels/filters\n","# being used in the operation. Essentially, the operation is taking the dot product\n","# of a single weight matrix (W_c) with each of the word vectors (l_t) from the\n","# query matrix (l_Q), adding a bias vector (b_c), and then applying the tanh activation.\n","# That is, h_Q = tanh(W_c • l_Q + b_c). With that being said, that's not actually\n","# how the operation is being calculated here. To tie the weights of the weight\n","# matrix (W_c) together, we have to use a one-dimensional convolutional layer. \n","# Further, we have to transpose our query matrix (l_Q) so that time is the first\n","# dimension rather than the second (as described in the paper). That is, l_Q[0, :]\n","# represents our first word vector rather than l_Q[:, 0]. We can think of the weight\n","# matrix (W_c) as being similarly transposed such that each kernel is a column\n","# of W_c. Therefore, h_Q = tanh(l_Q • W_c + b_c) with l_Q, W_c, and b_c being\n","# the transposes of the matrices described in the paper. Note: the paper does not\n","# include bias units.\n","query_conv = Convolution1D(K, FILTER_LENGTH, padding = \"same\", input_shape = (None, WORD_DEPTH), activation = \"tanh\")(query) # See equation (2).\n","\n","# Next, we apply a max-pooling layer to the convolved query matrix. Keras provides\n","# its own max-pooling layers, but they cannot handle variable length input (as\n","# far as I can tell). As a result, I define my own max-pooling layer here. In the\n","# paper, the operation selects the maximum value for each row of h_Q, but, because\n","# we're using the transpose, we're selecting the maximum value for each column.\n","query_max = Lambda(lambda x: backend.max(x, axis = 1), output_shape = (K, ))(query_conv) # See section 3.4.\n","\n","# In this step, we generate the semantic vector represenation of the query. This\n","# is a standard neural network dense layer, i.e., y = tanh(W_s • v + b_s). Again,\n","# the paper does not include bias units.\n","query_sem = Dense(L, activation = \"tanh\", input_dim = K)(query_max) # See section 3.5.\n","\n","# The document equivalent of the above query model.\n","doc_conv = Convolution1D(K, FILTER_LENGTH, padding = \"same\", input_shape = (None, WORD_DEPTH), activation = \"tanh\")\n","doc_max = Lambda(lambda x: backend.max(x, axis = 1), output_shape = (K, ))\n","doc_sem = Dense(L, activation = \"tanh\", input_dim = K)\n","\n","pos_doc_conv = doc_conv(pos_doc)\n","neg_doc_convs = [doc_conv(neg_doc) for neg_doc in neg_docs]\n","\n","pos_doc_max = doc_max(pos_doc_conv)\n","neg_doc_maxes = [doc_max(neg_doc_conv) for neg_doc_conv in neg_doc_convs]\n","\n","pos_doc_sem = doc_sem(pos_doc_max)\n","neg_doc_sems = [doc_sem(neg_doc_max) for neg_doc_max in neg_doc_maxes]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejplVT_QS1pw"},"source":["# This layer calculates the cosine similarity between the semantic representations of\n","# a query and a document.\n","R_Q_D_p = dot([query_sem, pos_doc_sem], axes = 1, normalize = True) # See equation (4).\n","R_Q_D_ns = [dot([query_sem, neg_doc_sem], axes = 1, normalize = True) for neg_doc_sem in neg_doc_sems] # See equation (4).\n","\n","concat_Rs = concatenate([R_Q_D_p] + R_Q_D_ns)\n","concat_Rs = Reshape((J + 1, 1))(concat_Rs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwKKkwXoS6Od"},"source":["# In this step, we multiply each R(Q, D) value by gamma. In the paper, gamma is\n","# described as a smoothing factor for the softmax function, and it's set empirically\n","# on a held-out data set. We're going to learn gamma's value by pretending it's\n","# a single 1 x 1 kernel.\n","weight = np.array([1]).reshape(1, 1, 1)\n","with_gamma = Convolution1D(1, 1, padding = \"same\", input_shape = (J + 1, 1), activation = \"linear\", use_bias = False, weights = [weight])(concat_Rs) # See equation (5).\n","with_gamma = Reshape((J + 1, ))(with_gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVIWnU_vS-NZ"},"source":["# Finally, we use the softmax function to calculate P(D+|Q).\n","prob = Activation(\"softmax\")(with_gamma) # See equation (5)."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l9TSxOcETCjd"},"source":["# We now have everything we need to define our model.\n","model = Model(inputs = [query, pos_doc] + neg_docs, outputs = prob)\n","model.compile(optimizer = \"adadelta\", loss = \"categorical_crossentropy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"77_SatqgTfR4"},"source":["# Build a random data set.\n","sample_size = 10\n","l_Qs = []\n","pos_l_Ds = []\n","\n","# Variable length input must be handled differently from padded input.\n","BATCH = True\n","\n","(query_len, doc_len) = (5, 100)\n","\n","for i in range(sample_size):\n","    \n","    if BATCH:\n","        l_Q = np.random.rand(query_len, WORD_DEPTH)\n","        l_Qs.append(l_Q)\n","        \n","        l_D = np.random.rand(doc_len, WORD_DEPTH)\n","        pos_l_Ds.append(l_D)\n","    else:\n","        query_len = np.random.randint(1, 10)\n","        l_Q = np.random.rand(1, query_len, WORD_DEPTH)\n","        l_Qs.append(l_Q)\n","        \n","        doc_len = np.random.randint(50, 500)\n","        l_D = np.random.rand(1, doc_len, WORD_DEPTH)\n","        pos_l_Ds.append(l_D)\n","\n","neg_l_Ds = [[] for j in range(J)]\n","for i in range(sample_size):\n","    possibilities = list(range(sample_size))\n","    possibilities.remove(i)\n","    negatives = np.random.choice(possibilities, J, replace = False)\n","    for j in range(J):\n","        negative = negatives[j]\n","        neg_l_Ds[j].append(pos_l_Ds[negative])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5_-VhosUJfv","executionInfo":{"status":"ok","timestamp":1636975174455,"user_tz":480,"elapsed":233,"user":{"displayName":"Jc Cearley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCYmKqYuS5h7Z0DaS97gM3-tV06kzbvE3d0cjs=s64","userId":"01766600583102153876"}},"outputId":"1960c3de-4540-4722-eacc-573bc51e9ac6"},"source":["print(l_Qs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[array([[0.36706951, 0.52622637, 0.19816423, ..., 0.77376525, 0.00798457,\n","        0.58824104],\n","       [0.1535122 , 0.55768901, 0.27422153, ..., 0.31347295, 0.82865904,\n","        0.42459496],\n","       [0.06668968, 0.08462038, 0.98609433, ..., 0.26303494, 0.00278939,\n","        0.68025221],\n","       [0.86205057, 0.89011927, 0.36435359, ..., 0.64311111, 0.97709922,\n","        0.7474956 ],\n","       [0.28649032, 0.84623481, 0.02422905, ..., 0.36247197, 0.37593486,\n","        0.86407487]]), array([[0.64135425, 0.57587251, 0.49996072, ..., 0.2522693 , 0.69645636,\n","        0.00707081],\n","       [0.90120471, 0.20968467, 0.64081788, ..., 0.35614704, 0.0511962 ,\n","        0.70570297],\n","       [0.34331818, 0.873949  , 0.58221335, ..., 0.05792189, 0.26605466,\n","        0.11527776],\n","       [0.5881895 , 0.06235405, 0.30071251, ..., 0.08735268, 0.57243133,\n","        0.10175501],\n","       [0.98027846, 0.53871257, 0.09363276, ..., 0.62775461, 0.87895528,\n","        0.04012251]]), array([[0.53967487, 0.68792822, 0.07435465, ..., 0.64161319, 0.68315976,\n","        0.45178089],\n","       [0.62557065, 0.07709444, 0.52445821, ..., 0.23148913, 0.72183761,\n","        0.21810845],\n","       [0.46061245, 0.0209045 , 0.9952    , ..., 0.23695261, 0.15801144,\n","        0.53281476],\n","       [0.32534874, 0.05403276, 0.85240091, ..., 0.29743073, 0.11934793,\n","        0.2293954 ],\n","       [0.08338152, 0.51406466, 0.49510605, ..., 0.90150805, 0.26326726,\n","        0.95687479]]), array([[0.85304154, 0.93579041, 0.68243527, ..., 0.07796535, 0.54997839,\n","        0.99302901],\n","       [0.85356706, 0.78393096, 0.33111055, ..., 0.89092108, 0.08547359,\n","        0.01098276],\n","       [0.68753123, 0.47065804, 0.0868765 , ..., 0.2228897 , 0.73210584,\n","        0.06642475],\n","       [0.90324311, 0.54579865, 0.85856016, ..., 0.10873964, 0.94480678,\n","        0.7470226 ],\n","       [0.34352122, 0.33061656, 0.75456562, ..., 0.70302495, 0.12842777,\n","        0.86512718]]), array([[0.42142889, 0.8270052 , 0.3052629 , ..., 0.88086665, 0.36170595,\n","        0.07983139],\n","       [0.95881938, 0.22536821, 0.77577955, ..., 0.39750022, 0.11926318,\n","        0.38463524],\n","       [0.19795732, 0.42358467, 0.71974312, ..., 0.82426945, 0.9947638 ,\n","        0.9514783 ],\n","       [0.04414794, 0.69018889, 0.66730626, ..., 0.41664083, 0.92838723,\n","        0.97945131],\n","       [0.19461001, 0.67815224, 0.55582764, ..., 0.8117174 , 0.05843311,\n","        0.63037429]]), array([[0.07656314, 0.41296882, 0.66364011, ..., 0.87167955, 0.93708378,\n","        0.24098382],\n","       [0.27546419, 0.01969077, 0.03216115, ..., 0.89823159, 0.81819816,\n","        0.25276374],\n","       [0.83420137, 0.02758598, 0.79242972, ..., 0.81816211, 0.511756  ,\n","        0.00551157],\n","       [0.54761873, 0.67285566, 0.24456143, ..., 0.2296997 , 0.1108371 ,\n","        0.16097774],\n","       [0.6746607 , 0.2764148 , 0.5235888 , ..., 0.27926584, 0.61311831,\n","        0.85555873]]), array([[0.26127795, 0.46962195, 0.14267451, ..., 0.17359384, 0.45323191,\n","        0.32664985],\n","       [0.90172859, 0.35718253, 0.54594204, ..., 0.12797889, 0.88766461,\n","        0.59126321],\n","       [0.34734739, 0.21319618, 0.55129219, ..., 0.15607343, 0.39817563,\n","        0.1482774 ],\n","       [0.27337509, 0.15393584, 0.26774291, ..., 0.7191571 , 0.6563426 ,\n","        0.84033353],\n","       [0.76283958, 0.1142364 , 0.51045838, ..., 0.5215576 , 0.3449563 ,\n","        0.60405036]]), array([[0.96570076, 0.17248287, 0.56318612, ..., 0.09909827, 0.87403592,\n","        0.91704685],\n","       [0.68314539, 0.10597326, 0.94320498, ..., 0.24247758, 0.96727883,\n","        0.97940403],\n","       [0.29533595, 0.90636754, 0.2083442 , ..., 0.51339836, 0.39243471,\n","        0.60273688],\n","       [0.96244522, 0.08566051, 0.22137924, ..., 0.02811121, 0.7220802 ,\n","        0.96484054],\n","       [0.92468769, 0.84504829, 0.37093225, ..., 0.19423822, 0.28971452,\n","        0.26178341]]), array([[0.29118973, 0.45906416, 0.71742874, ..., 0.15030459, 0.69206888,\n","        0.42902807],\n","       [0.00981921, 0.07461442, 0.96253104, ..., 0.2172138 , 0.23928091,\n","        0.36929387],\n","       [0.37530047, 0.58115791, 0.8018297 , ..., 0.64894356, 0.85321809,\n","        0.63631346],\n","       [0.56824067, 0.67319414, 0.40943782, ..., 0.5257792 , 0.1992091 ,\n","        0.84881712],\n","       [0.72606872, 0.62980544, 0.34201745, ..., 0.73889588, 0.08973865,\n","        0.21158588]]), array([[0.41305915, 0.41968188, 0.3154681 , ..., 0.04559425, 0.22498358,\n","        0.07363082],\n","       [0.52406734, 0.65345678, 0.65888444, ..., 0.96418139, 0.77330484,\n","        0.6160406 ],\n","       [0.64696678, 0.7046574 , 0.74397755, ..., 0.6023487 , 0.13750439,\n","        0.96165343],\n","       [0.99992056, 0.22761128, 0.65067448, ..., 0.98121039, 0.0757442 ,\n","        0.58668142],\n","       [0.42189478, 0.59195052, 0.27429684, ..., 0.67869564, 0.69806023,\n","        0.75956488]])]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"7ybszSesTgDl","executionInfo":{"status":"error","timestamp":1636976389075,"user_tz":480,"elapsed":230,"user":{"displayName":"Jc Cearley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCYmKqYuS5h7Z0DaS97gM3-tV06kzbvE3d0cjs=s64","userId":"01766600583102153876"}},"outputId":"5167514d-ad11-43eb-a1b1-50223703dcff"},"source":["\n","\n","if BATCH:\n","    y = np.zeros((sample_size, J + 1))\n","    y[:, 0] = 1\n","    \n","    l_Qs = np.array(l_Qs)\n","    pos_l_Ds = np.array(pos_l_Ds)\n","    for j in range(J):\n","        neg_l_Ds[j] = np.array(neg_l_Ds[j])\n","    \n","    history = model.fit([l_Qs, pos_l_Ds] + [neg_l_Ds[j] for j in range(J)], y, epochs = 1, verbose = 0)\n","else:\n","    y = np.zeros(J + 1).reshape(1, J + 1)\n","    y[0, 0] = 1\n","    \n","    for i in range(sample_size):\n","        history = model.fit([l_Qs[i], pos_l_Ds[i]] + [neg_l_Ds[j][i] for j in range(J)], y, epochs = 1, verbose = 0)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8d6a43e6671a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mBATCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BATCH' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GF8GE5jyTkKC"},"source":["# Here, I walk through how to define a function for calculating output from the\n","# computational graph. Let's define a function that calculates R(Q, D+) for a given\n","# query and clicked document. The function depends on two inputs, query and pos_doc.\n","# That is, if you start at the point in the graph where R(Q, D+) is calculated\n","# and then work backwards as far as possible, you'll end up at two different starting\n","# points: query and pos_doc. As a result, we supply those inputs in a list to the\n","# function. This particular function only calculates a single output, but multiple\n","# outputs are possible (see the next example).\n","get_R_Q_D_p = backend.function([query, pos_doc], [R_Q_D_p])\n","if BATCH:\n","    get_R_Q_D_p([l_Qs, pos_l_Ds])\n","else:\n","    get_R_Q_D_p([l_Qs[0], pos_l_Ds[0]])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mipay-qTnYS"},"source":["# A slightly more complex function. Notice that both neg_docs and the output are\n","# lists.\n","get_R_Q_D_ns = backend.function([query] + neg_docs, R_Q_D_ns)\n","if BATCH:\n","    get_R_Q_D_ns([l_Qs] + [neg_l_Ds[j] for j in range(J)])\n","else:\n","    get_R_Q_D_ns([l_Qs[0]] + neg_l_Ds[0])"],"execution_count":null,"outputs":[]}]}